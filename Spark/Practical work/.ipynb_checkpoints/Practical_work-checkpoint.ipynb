{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MrBC530gmg5"
   },
   "source": [
    "Let's start with your project: \n",
    "\n",
    "Are you a data scientist? \n",
    "\n",
    "I think you are an awesome a data scientist.\n",
    "\n",
    "### **Problem** \n",
    "**Our goal is to create a predictive model that can answer the following question:**\n",
    "\n",
    "**What kind of people had a better chance of surviving?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qhBfwzBgioTD"
   },
   "source": [
    "**Data about passengers:**\n",
    "*   Name\n",
    "*   Age\n",
    "*   Gender.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIEH8iZqi-sk"
   },
   "source": [
    "## Install and Import Libraries\n",
    "Let's install PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oj1HhvIOY5Yz"
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDp80mG9jmfU"
   },
   "source": [
    "## Build Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ttzML9fpjE5a"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TiqECDzLj1Mg"
   },
   "source": [
    "## Data Loading\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vn-hxNggkTqV"
   },
   "source": [
    "You have two datasets: \n",
    "* Train  \n",
    "* Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8-A8M7QmKDJ"
   },
   "source": [
    "Read two datasets: \n",
    "* Train\n",
    "* Test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Mx2qAccBk15y"
   },
   "outputs": [],
   "source": [
    "train_df = spark.read.csv('train.csv' ,header = True , inferSchema =True)\n",
    "test_df = spark.read.csv('test.csv' ,header = True , inferSchema =True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj2ANTnWmSCq"
   },
   "source": [
    "Let's work with train dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5mWJR30lNs5"
   },
   "source": [
    "**Confirm if this is a dataframe or not:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tEYTePrzk9yl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvLJElPrlT4i"
   },
   "source": [
    "**Show 5 rows.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jYwhqvV8lnO0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QIYVxRXlnnw"
   },
   "source": [
    "**Display schema for the dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "pcvERiICl1Ep"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmE3Wd80l1S6"
   },
   "source": [
    "**Statistical summary:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "cNY0SItol5Mo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                null|  null| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| null|    null|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                null|  null|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| null|    null|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    25%|              223|                  0|                 2|                null|  null|              20.0|                 0|                  0|           19996.0|           7.8958| null|    null|\n",
      "|    50%|              446|                  0|                 3|                null|  null|              28.0|                 0|                  0|          236171.0|          14.4542| null|    null|\n",
      "|    75%|              669|                  1|                 3|                null|  null|              38.0|                 1|                  0|          347743.0|             31.0| null|    null|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiFaIEQTl70_"
   },
   "source": [
    "## EDA - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSNPOnP8mw2Q"
   },
   "source": [
    "**Display count for the train dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zrtpG11Fl9HM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "891\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "print(train_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_6nnTfxm9_x"
   },
   "source": [
    "**Can you answer this question:** \n",
    "\n",
    "**How many people survived, and how many didn't survive?** \n",
    "\n",
    "**Please save data in a variable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QDoqPwyomYxA"
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "cnt_cond = lambda cond: F.sum(F.when(cond, 1).otherwise(0))\n",
    "query_survive = train_df.groupBy('Survived').agg(\n",
    "    cnt_cond(F.col('Survived') == 1).alias('Survived_one'), \n",
    "    cnt_cond(F.col('Survived') == 0).alias('not_Survived_one'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P8DUtZXPn46m"
   },
   "source": [
    "**Display your result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0XHAK8ceoCMU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+----------------+\n",
      "|Survived|Survived_one|not_Survived_one|\n",
      "+--------+------------+----------------+\n",
      "|       1|         342|               0|\n",
      "|       0|           0|             549|\n",
      "+--------+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query_survive.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby('Survived').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ygsg7wQqor9a"
   },
   "source": [
    "**Can you display your answer in ratio form?(Hint: Use \"UDF\" Function. (Hint: Use \"UDF\" Function. This is a hint you can use any method.)**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3uiaN29PoQnf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ratio of 342.0 and 549.0 is 0.6229508196721312 .\n"
     ]
    }
   ],
   "source": [
    "def ratioFunction(num1, num2):\n",
    "    num1 = float(num1) # Now we are good\n",
    "    num2 = float(num2) # Good, good\n",
    "    ratio12 = float(num1/num2)\n",
    "    print('The ratio of', num1, 'and', num2,'is', ratio12 , '.')\n",
    "ratioFunction(342, 549)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7Aker_lp1h4"
   },
   "source": [
    "**Can you get the number of males and females?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XllkDlo3ongJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+\n",
      "|   Sex|male|female|\n",
      "+------+----+------+\n",
      "|female|   0|   314|\n",
      "|  male| 577|     0|\n",
      "+------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnt_cond = lambda cond: F.sum(F.when(cond, 1).otherwise(0))\n",
    "train_df.groupBy('Sex').agg(\n",
    "    cnt_cond(F.col('Sex') == 'male').alias('male'), \n",
    "    cnt_cond(F.col('Sex') == 'female').alias('female')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  314|\n",
      "|  male|  577|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby('Sex').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHFaJ15zqtEV"
   },
   "source": [
    "**1. What is the average number of survivors of each gender?**\n",
    "\n",
    "**2. What is the number of survivors of each gender?**\n",
    "\n",
    "(Hint: Group by the \"sex\" column. This is a hint you can use any method.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NUikH7MUqdKq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  314|\n",
      "|  male|  577|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby('Sex').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kCEdYNdArtRN"
   },
   "source": [
    "**Create temporary view PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "YjlK6HDUqsI5"
   },
   "outputs": [],
   "source": [
    "train_df.createOrReplaceTempView(\"practical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXNePifnshHr"
   },
   "source": [
    "**How many people survived, and how many didn't survive? By SQL:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "0HxfPRTMslqk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(Survived)|\n",
      "+---------------+\n",
      "|            342|\n",
      "|            549|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(Survived)\n",
    "  FROM practical\n",
    " GROUP BY Survived\n",
    "        \"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sVCdY6EasFWV"
   },
   "source": [
    "**Can you display the number of survivors from each gender as a ratio?**\n",
    "\n",
    "(Hint: Group by \"sex\" column. This is a hint you can use any method.)\n",
    "\n",
    "**Can you do this via SQL?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7xQc3pUUr3HF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  314|\n",
      "|  male|  577|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupby('Sex').count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|count(Sex)|\n",
      "+----------+\n",
      "|       314|\n",
      "|       577|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT count(Sex)\n",
    "  FROM practical\n",
    " GROUP BY Sex\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6QXc5V8uu3Y"
   },
   "source": [
    "**Display a ratio for \"p-class\": SUM(Survived)/count for p-class**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Mscs2mDFdFsD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------------------------------------------------------------------+\n",
      "|sum(Survived)|(CAST(sum(CAST(Survived AS BIGINT)) AS DOUBLE) / CAST(count(Pclass) AS DOUBLE))|\n",
      "+-------------+-------------------------------------------------------------------------------+\n",
      "|          136|                                                             0.6296296296296297|\n",
      "|          119|                                                            0.24236252545824846|\n",
      "|           87|                                                            0.47282608695652173|\n",
      "+-------------+-------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark.sql(\"\"\"SELECT sum(Survived) , sum(Survived) / count(Pclass) \n",
    "  FROM practical\n",
    " GROUP BY Pclass\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EX0klxwAvg6J"
   },
   "source": [
    "**Let's take a break and continue after this.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ctM9t8atxJl"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CfanZTCt6Wk"
   },
   "source": [
    "**First and foremost, we must merge both the train and test datasets. (Hint: The union function can do this.)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "8Nm8S1K0r4uY"
   },
   "outputs": [],
   "source": [
    "from functools import reduce  \n",
    "from pyspark.sql import DataFrame\n",
    "  \n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.unionAll, dfs)\n",
    "  \n",
    "full_df = unionAll(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jI7AD8FLz3iO"
   },
   "source": [
    "**Display count:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "s_WERAL8wvJa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1329"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R4Miuy0z_uP"
   },
   "source": [
    "**Can you define the number of null values in each column?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0LMOalKBxhpD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|265|    0|    0|     0|   0| 1021|       3|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "null_val =full_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in full_df.columns])\n",
    "null_val.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBX8cJ000aqe"
   },
   "source": [
    "**Create Dataframe for null values**\n",
    "\n",
    "1. Column\n",
    "2. Number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ITmyUelNxjJM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------+\n",
      "|Age|Cabin|Embarked|\n",
      "+---+-----+--------+\n",
      "|265| 1021|       3|\n",
      "+---+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_val.createOrReplaceTempView(\"Select_Null\")\n",
    "spark.sql(\"\"\"SELECT Age , Cabin , Embarked\n",
    "  FROM Select_Null\n",
    "        \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuKrOi5a0-Ma"
   },
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVQlr9vDy7Y4"
   },
   "source": [
    "**Create Temporary view PySpark:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xs3yeXhGI8rv"
   },
   "outputs": [],
   "source": [
    "full_df.createOrReplaceTempView(\"new_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txa8NZIO1JaP"
   },
   "source": [
    "**Can you show the \"name\" column from your temporary table?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "m7yXqJoJy35k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|name                                               |\n",
      "+---------------------------------------------------+\n",
      "|Braund, Mr. Owen Harris                            |\n",
      "|Cumings, Mrs. John Bradley (Florence Briggs Thayer)|\n",
      "|Heikkinen, Miss. Laina                             |\n",
      "|Futrelle, Mrs. Jacques Heath (Lily May Peel)       |\n",
      "|Allen, Mr. William Henry                           |\n",
      "+---------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT name\n",
    "  FROM new_temp\n",
    "        \"\"\").show(5,truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3F0F9cTZ2Cuz"
   },
   "source": [
    "**Run this code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "0kx6OcB-2BBT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Title|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|   Mr|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|  Mrs|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S| Miss|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|  Mrs|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|   Mr|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "combined = full_df.withColumn('Title',F.regexp_extract(F.col(\"Name\"),\"([A-Za-z]+)\\.\",1))\n",
    "combined.createOrReplaceTempView('combined')\n",
    "combined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xbZeUWS12r59"
   },
   "source": [
    "**Display \"Title\" column and count \"Title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "hGkFMtlp1FAI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Title   |Titles_Count|\n",
      "+--------+------------+\n",
      "|Mme     |1           |\n",
      "|Don     |1           |\n",
      "|Ms      |1           |\n",
      "|Countess|2           |\n",
      "|Jonkheer|2           |\n",
      "|Sir     |2           |\n",
      "|Lady    |2           |\n",
      "|Capt    |2           |\n",
      "|Major   |3           |\n",
      "|Mlle    |4           |\n",
      "|Col     |4           |\n",
      "|Rev     |9           |\n",
      "|Dr      |11          |\n",
      "|Master  |56          |\n",
      "|Mrs     |186         |\n",
      "|Miss    |257         |\n",
      "|Mr      |786         |\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Titles_df = spark.sql(\"\"\"SELECT Title , COUNT(Title) as Titles_Count\n",
    "  FROM combined\n",
    "  GROUP BY Title\n",
    "  ORDER BY Titles_Count\n",
    "        \"\"\")\n",
    "Titles_df.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLBQDKYu4JOa"
   },
   "source": [
    "**We can see that Dr, Rev, Major, Col, Mlle, Capt, Don, Jonkheer, Countess, Ms, Sir, Lady, and Mme are really rare titles, so create Dictionary and set the value to \"rare\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "rjnx5l5r2Qaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Don': 'RaRe',\n",
       " 'Mme': 'RaRe',\n",
       " 'Ms': 'RaRe',\n",
       " 'Countess': 'RaRe',\n",
       " 'Lady': 'RaRe',\n",
       " 'Capt': 'RaRe',\n",
       " 'Sir': 'RaRe',\n",
       " 'Jonkheer': 'RaRe',\n",
       " 'Major': 'RaRe',\n",
       " 'Col': 'RaRe',\n",
       " 'Mlle': 'RaRe',\n",
       " 'Rev': 'RaRe',\n",
       " 'Dr': 'RaRe',\n",
       " 'Master': 'Master',\n",
       " 'Mrs': 'Mrs',\n",
       " 'Miss': 'Miss',\n",
       " 'Mr': 'Mr'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict = list(map (lambda row : row.asDict(),Titles_df.collect()))\n",
    "dict_map = {row['Title']:'RaRe' if row['Titles_Count'] < 56 else row['Title'] for row in new_dict}\n",
    "dict_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wrE95Cv7Oqh"
   },
   "source": [
    "**Run the function:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "HdDbWuDl7Pf4"
   },
   "outputs": [],
   "source": [
    "def impute_title(title):\n",
    "    return dict_map[title]# Title_map is your dictionary. please change this name with your dictionary name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f5EQVIhK7a9R"
   },
   "source": [
    "**Apply the function on \"Title\" column using UDF:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "rBAiIOn77XFa"
   },
   "outputs": [],
   "source": [
    "# import org.apache.spark.sql.functions.udf package\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "convertUDF = udf(lambda z: impute_title(z))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sn8ewllf7kiV"
   },
   "source": [
    "**Display \"Title\" from table and group by \"Title\" column:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "J9sjQb084GU6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|Title |Title |\n",
      "+------+------+\n",
      "|Mr    |Mr    |\n",
      "|Mrs   |Mrs   |\n",
      "|Miss  |Miss  |\n",
      "|Mrs   |Mrs   |\n",
      "|Mr    |Mr    |\n",
      "|Mr    |Mr    |\n",
      "|Mr    |Mr    |\n",
      "|Master|Master|\n",
      "|Mrs   |Mrs   |\n",
      "|Mrs   |Mrs   |\n",
      "|Miss  |Miss  |\n",
      "|Miss  |Miss  |\n",
      "|Mr    |Mr    |\n",
      "|Mr    |Mr    |\n",
      "|Miss  |Miss  |\n",
      "|Mrs   |Mrs   |\n",
      "|Master|Master|\n",
      "|Mr    |Mr    |\n",
      "|Mrs   |Mrs   |\n",
      "|Mrs   |Mrs   |\n",
      "+------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# using select\n",
    "combined.select(F.col(\"Title\"), \\\n",
    "    convertUDF(F.col(\"Title\")).alias(\"Title\") ) \\\n",
    "   .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H45QNLj9vJp"
   },
   "source": [
    "## **Preprocessing Age**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwRAhumK-u__"
   },
   "source": [
    "**Based on the \"age\" column mean, you will fill in the missing age values:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "eXYSVzvl4z63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          avg(Age)|\n",
      "+------------------+\n",
      "|30.079501879699244|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean , col \n",
    "\n",
    "Age_mean = full_df.select(mean(col('Age')))\n",
    "Age_mean.show()\n",
    "df_age =full_df.na.fill({'age': 30.079501879699244})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLPivde8_GI-"
   },
   "source": [
    "**Fill missing with \"age\" mean:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "lBgW8aFD90PA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_age.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGsnUz-m_P95"
   },
   "source": [
    "## **Preprocessing Embarked**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHbbamcXMSYP"
   },
   "source": [
    "**Select \"Embarked\" column, count them, order by count Desc, and save in grouped_Embarked variable:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "v-lRu5vc_FW7"
   },
   "outputs": [],
   "source": [
    "df_age.createOrReplaceTempView(\"Embarked\")\n",
    "groupped_Embarked = spark.sql(\"\"\"SELECT Embarked , count(Embarked) as Embark_count\n",
    "  FROM Embarked\n",
    "  group by Embarked\n",
    "  order by Embark_count DESC\n",
    "        \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E1qf5u2IOQrx"
   },
   "source": [
    "**Show \"groupped_Embarked\" your variable:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "jSFNDTNg_erb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|Embarked|Embark_count|\n",
      "+--------+------------+\n",
      "|S       |962         |\n",
      "|C       |253         |\n",
      "|Q       |111         |\n",
      "|null    |0           |\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupped_Embarked.show(5,truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzQWYgKBMrbp"
   },
   "source": [
    "**Get max of groupped_Embarked:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "ZLYj4F7E_iqb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|max(Embark_count)|\n",
      "+-----------------+\n",
      "|              962|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "max_empark = groupped_Embarked.select(max(col('Embark_count')))\n",
    "max_empark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8vhoEs8N2w_"
   },
   "source": [
    "**Fill missing values with max 'S' of grouped_Embarked:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LdzQCRud_mAa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEcdV5Vb_qR_"
   },
   "source": [
    "## **Preprocessing Cabin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BQzPs7tqhpA"
   },
   "source": [
    "**Replace \"cabin\" column with first char from the string:**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "4b6L5pK0_nQz"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, substring\n",
    "#df1=df_age.select('cabin', substring('cabin', 0,1).alias('cabin_First_string'))\n",
    "\n",
    "df1 = df_age.withColumn(\"cabin_First_string\", substring('cabin', 0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6H8XshnYj4k2"
   },
   "source": [
    "**Show the result:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "gJUQwnG1Oj2U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+----------------+-------+-----+--------+------------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|               Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|cabin_First_string|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+----------------+-------+-----+--------+------------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|              22.0|    1|    0|       A/5 21171|   7.25| null|       S|              null|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|              38.0|    1|    0|        PC 17599|71.2833|  C85|       C|                 C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|              26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|              null|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|              35.0|    1|    0|          113803|   53.1| C123|       S|                 C|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|              35.0|    0|    0|          373450|   8.05| null|       S|              null|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|30.079501879699244|    0|    0|          330877| 8.4583| null|       Q|              null|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|              54.0|    0|    0|           17463|51.8625|  E46|       S|                 E|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male|               2.0|    3|    1|          349909| 21.075| null|       S|              null|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|              27.0|    0|    2|          347742|11.1333| null|       S|              null|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|              14.0|    1|    0|          237736|30.0708| null|       C|              null|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|               4.0|    1|    1|         PP 9549|   16.7|   G6|       S|                 G|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|              58.0|    0|    0|          113783|  26.55| C103|       S|                 C|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|              20.0|    0|    0|       A/5. 2151|   8.05| null|       S|              null|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|              39.0|    1|    5|          347082| 31.275| null|       S|              null|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|              14.0|    0|    0|          350406| 7.8542| null|       S|              null|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|              55.0|    0|    0|          248706|   16.0| null|       S|              null|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male|               2.0|    4|    1|          382652| 29.125| null|       Q|              null|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|30.079501879699244|    0|    0|          244373|   13.0| null|       S|              null|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|              31.0|    1|    0|          345763|   18.0| null|       S|              null|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|30.079501879699244|    0|    0|            2649|  7.225| null|       C|              null|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+----------------+-------+-----+--------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzSDsWsUj9Im"
   },
   "source": [
    "**Create the temporary view:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "MR7CXTY7_tMJ"
   },
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"cabin_First_string\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gv7lfQFkrLlN"
   },
   "source": [
    "**Select \"Cabin\" column, count \"Cabin\" column, Group by \"Cabin\" column, Order By count DESC**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "A0tZG_mvrKXv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|      Cabin|Cabin_count|\n",
      "+-----------+-----------+\n",
      "|    B96 B98|          6|\n",
      "|    C22 C26|          4|\n",
      "|        D20|          4|\n",
      "|        B22|          4|\n",
      "|B51 B53 B55|          4|\n",
      "+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "groupped_cabin = spark.sql(\"\"\"SELECT Cabin , count(Cabin) as Cabin_count\n",
    "  FROM cabin_First_string\n",
    "  group by Cabin\n",
    "  order by Cabin_count DESC\n",
    "        \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GR6j0LOsB4y"
   },
   "source": [
    "**Fill missing values with \"U\":**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "mwq5CHEz_up_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-----+--------+------------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|  Ticket|   Fare|Cabin|Embarked|cabin_First_string|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-----+--------+------------------+\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|PC 17599|71.2833|  C85|       C|                 C|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|  113803|   53.1| C123|       S|                 C|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|   17463|51.8625|  E46|       S|                 E|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1| PP 9549|   16.7|   G6|       S|                 G|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|  113783|  26.55| C103|       S|                 C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+--------+-------+-----+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 =df1.na.fill({'cabin_First_string': 'U'})\n",
    "df2 = df2.na.drop()\n",
    "df2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRnhA_5-0Hi4"
   },
   "source": [
    "**StringIndexer: A label indexer that maps a string column of labels to an ML column of label indices. If the input column is numeric, we cast it to string and index the string values. The indices are in [0, numLabels). By default, this is ordered by label frequencies so the most frequent label gets index 0. The ordering behavior is controlled by setting stringOrderType. Its default value is ‘frequencyDesc’.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1RIKlOX71GQ-"
   },
   "source": [
    "**StringIndexer(inputCol=None, outputCol=None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "odhuI2EHKuCm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+------------------+-----+-----+--------+-----------+--------------+----------------+--------------------------+\n",
      "|PassengerId|Survived|Pclass|               Age|SibSp|Parch|    Fare|Sex_numeric|Ticket_numeric|Embarked_numeric|cabin_First_string_numeric|\n",
      "+-----------+--------+------+------------------+-----+-----+--------+-----------+--------------+----------------+--------------------------+\n",
      "|          2|       1|     1|              38.0|    1|    0| 71.2833|        1.0|         134.0|             1.0|                       0.0|\n",
      "|          4|       1|     1|              35.0|    1|    0|    53.1|        1.0|          48.0|             0.0|                       0.0|\n",
      "|          7|       0|     1|              54.0|    0|    0| 51.8625|        0.0|         118.0|             0.0|                       3.0|\n",
      "|         11|       1|     3|               4.0|    1|    1|    16.7|        1.0|          95.0|             0.0|                       6.0|\n",
      "|         12|       1|     1|              58.0|    0|    0|   26.55|        1.0|         111.0|             0.0|                       0.0|\n",
      "|         22|       1|     2|              34.0|    0|    0|    13.0|        0.0|         123.0|             0.0|                       2.0|\n",
      "|         24|       1|     1|              28.0|    0|    0|    35.5|        0.0|         114.0|             0.0|                       4.0|\n",
      "|         28|       0|     1|              19.0|    3|    2|   263.0|        0.0|          12.0|             0.0|                       0.0|\n",
      "|         32|       1|     1|30.079501879699244|    1|    0|146.5208|        1.0|          89.0|             1.0|                       1.0|\n",
      "|         53|       1|     1|              49.0|    1|    0| 76.7292|        1.0|           6.0|             1.0|                       2.0|\n",
      "|         55|       0|     1|              65.0|    0|    1| 61.9792|        0.0|         107.0|             1.0|                       1.0|\n",
      "|         56|       1|     1|30.079501879699244|    0|    0|    35.5|        0.0|         120.0|             0.0|                       0.0|\n",
      "|         63|       0|     1|              45.0|    1|    0|  83.475|        0.0|          76.0|             0.0|                       0.0|\n",
      "|         67|       1|     2|              29.0|    0|    0|    10.5|        1.0|         131.0|             0.0|                       5.0|\n",
      "|         76|       0|     3|              25.0|    0|    0|    7.65|        0.0|         129.0|             0.0|                       5.0|\n",
      "|         89|       1|     1|              23.0|    3|    2|   263.0|        1.0|          12.0|             0.0|                       0.0|\n",
      "|         93|       0|     1|              46.0|    1|    0|  61.175|        0.0|         140.0|             0.0|                       3.0|\n",
      "|         97|       0|     1|              71.0|    0|    0| 34.6542|        0.0|         136.0|             1.0|                       4.0|\n",
      "|         98|       1|     1|              23.0|    0|    1| 63.3583|        0.0|         137.0|             1.0|                       2.0|\n",
      "|        103|       0|     1|              21.0|    0|    1| 77.2875|        0.0|          73.0|             0.0|                       2.0|\n",
      "+-----------+--------+------+------------------+-----+-----+--------+-----------+--------------+----------------+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCols =[ \"Sex\" , \"Ticket\" ,\"Embarked\" , \"cabin_First_string\"], outputCols=[\"Sex_numeric\" , \"Ticket_numeric\",\"Embarked_numeric\" , \"cabin_First_string_numeric\"]).fit(df2)\n",
    "indexed_df = indexer.transform(df2)\n",
    "indexed_df.drop(\"Sex\", \"Ticket\",\"Embarked\" , \"cabin_First_string\",\"Name\",\"Cabin\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1DECL9yCK3JZ"
   },
   "source": [
    "**OneHotEncoder(inputCols=None, outputCols=None)**\n",
    "\n",
    "A one-hot encoder that maps a column of category indices to a column of binary vectors, with at most a single one-value per row that indicates the input category index. For example with 5 categories, an input value of 2.0 would map to an output vector of [0.0, 0.0, 1.0, 0.0]. The last category is not included by default (configurable via dropLast), because it makes the vector entries sum up to one, and hence linearly dependent. So an input value of 4.0 maps to [0.0, 0.0, 0.0, 0.0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "tiAjDEy1LBhz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-----------+--------+-----------+--------+------------------+-------------+-----------------+----------------+-------------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex|               Age|SibSp|Parch|     Ticket|    Fare|      Cabin|Embarked|cabin_First_string|  encoded_Sex|   encoded_Ticket|encoded_Embarked|encoded_cabin_First|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-----------+--------+-----------+--------+------------------+-------------+-----------------+----------------+-------------------+\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|              38.0|    1|    0|   PC 17599| 71.2833|        C85|       C|                 C|(2,[1],[1.0])|(141,[134],[1.0])|   (3,[1],[1.0])|      (8,[0],[1.0])|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|              35.0|    1|    0|     113803|    53.1|       C123|       S|                 C|(2,[1],[1.0])| (141,[48],[1.0])|   (3,[0],[1.0])|      (8,[0],[1.0])|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|              54.0|    0|    0|      17463| 51.8625|        E46|       S|                 E|(2,[0],[1.0])|(141,[118],[1.0])|   (3,[0],[1.0])|      (8,[3],[1.0])|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female|               4.0|    1|    1|    PP 9549|    16.7|         G6|       S|                 G|(2,[1],[1.0])| (141,[95],[1.0])|   (3,[0],[1.0])|      (8,[6],[1.0])|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|              58.0|    0|    0|     113783|   26.55|       C103|       S|                 C|(2,[1],[1.0])|(141,[111],[1.0])|   (3,[0],[1.0])|      (8,[0],[1.0])|\n",
      "|         22|       1|     2|Beesley, Mr. Lawr...|  male|              34.0|    0|    0|     248698|    13.0|        D56|       S|                 D|(2,[0],[1.0])|(141,[123],[1.0])|   (3,[0],[1.0])|      (8,[2],[1.0])|\n",
      "|         24|       1|     1|Sloper, Mr. Willi...|  male|              28.0|    0|    0|     113788|    35.5|         A6|       S|                 A|(2,[0],[1.0])|(141,[114],[1.0])|   (3,[0],[1.0])|      (8,[4],[1.0])|\n",
      "|         28|       0|     1|Fortune, Mr. Char...|  male|              19.0|    3|    2|      19950|   263.0|C23 C25 C27|       S|                 C|(2,[0],[1.0])| (141,[12],[1.0])|   (3,[0],[1.0])|      (8,[0],[1.0])|\n",
      "|         32|       1|     1|Spencer, Mrs. Wil...|female|30.079501879699244|    1|    0|   PC 17569|146.5208|        B78|       C|                 B|(2,[1],[1.0])| (141,[89],[1.0])|   (3,[1],[1.0])|      (8,[1],[1.0])|\n",
      "|         53|       1|     1|Harper, Mrs. Henr...|female|              49.0|    1|    0|   PC 17572| 76.7292|        D33|       C|                 D|(2,[1],[1.0])|  (141,[6],[1.0])|   (3,[1],[1.0])|      (8,[2],[1.0])|\n",
      "|         55|       0|     1|Ostby, Mr. Engelh...|  male|              65.0|    0|    1|     113509| 61.9792|        B30|       C|                 B|(2,[0],[1.0])|(141,[107],[1.0])|   (3,[1],[1.0])|      (8,[1],[1.0])|\n",
      "|         56|       1|     1|   Woolner, Mr. Hugh|  male|30.079501879699244|    0|    0|      19947|    35.5|        C52|       S|                 C|(2,[0],[1.0])|(141,[120],[1.0])|   (3,[0],[1.0])|      (8,[0],[1.0])|\n",
      "|         63|       0|     1|Harris, Mr. Henry...|  male|              45.0|    1|    0|      36973|  83.475|        C83|       S|                 C|(2,[0],[1.0])| (141,[76],[1.0])|   (3,[0],[1.0])|      (8,[0],[1.0])|\n",
      "|         67|       1|     2|Nye, Mrs. (Elizab...|female|              29.0|    0|    0| C.A. 29395|    10.5|        F33|       S|                 F|(2,[1],[1.0])|(141,[131],[1.0])|   (3,[0],[1.0])|      (8,[5],[1.0])|\n",
      "|         76|       0|     3|Moen, Mr. Sigurd ...|  male|              25.0|    0|    0|     348123|    7.65|      F G73|       S|                 F|(2,[0],[1.0])|(141,[129],[1.0])|   (3,[0],[1.0])|      (8,[5],[1.0])|\n",
      "|         89|       1|     1|Fortune, Miss. Ma...|female|              23.0|    3|    2|      19950|   263.0|C23 C25 C27|       S|                 C|(2,[1],[1.0])| (141,[12],[1.0])|   (3,[0],[1.0])|      (8,[0],[1.0])|\n",
      "|         93|       0|     1|Chaffee, Mr. Herb...|  male|              46.0|    1|    0|W.E.P. 5734|  61.175|        E31|       S|                 E|(2,[0],[1.0])|(141,[140],[1.0])|   (3,[0],[1.0])|      (8,[3],[1.0])|\n",
      "|         97|       0|     1|Goldschmidt, Mr. ...|  male|              71.0|    0|    0|   PC 17754| 34.6542|         A5|       C|                 A|(2,[0],[1.0])|(141,[136],[1.0])|   (3,[1],[1.0])|      (8,[4],[1.0])|\n",
      "|         98|       1|     1|Greenfield, Mr. W...|  male|              23.0|    0|    1|   PC 17759| 63.3583|    D10 D12|       C|                 D|(2,[0],[1.0])|(141,[137],[1.0])|   (3,[1],[1.0])|      (8,[2],[1.0])|\n",
      "|        103|       0|     1|White, Mr. Richar...|  male|              21.0|    0|    1|      35281| 77.2875|        D26|       S|                 D|(2,[0],[1.0])| (141,[73],[1.0])|   (3,[0],[1.0])|      (8,[2],[1.0])|\n",
      "+-----------+--------+------+--------------------+------+------------------+-----+-----+-----------+--------+-----------+--------+------------------+-------------+-----------------+----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCols=[\"Sex_numeric\" , \"Ticket_numeric\",\"Embarked_numeric\" , \"cabin_First_string_numeric\"], outputCols=[\"encoded_Sex\" , \"encoded_Ticket\",\"encoded_Embarked\" , \"encoded_cabin_First\"])\n",
    "encoder.setDropLast(False)\n",
    "ohe = encoder.fit(indexed_df)\n",
    "encoded_df = ohe.transform(indexed_df)\n",
    "encoded_df.drop(\"Sex_numeric\" , \"Ticket_numeric\",\"Embarked_numeric\" , \"cabin_First_string_numeric\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FsiLsd9452v"
   },
   "source": [
    "**VectorAssembler: VectorAssembler(*, inputCols=None, outputCol=None). A feature transformer that merges multiple columns into a vector column.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BuytKk0hLE6p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+--------+\n",
      "|features                                                                          |Survived|\n",
      "+----------------------------------------------------------------------------------+--------+\n",
      "|(159,[0,1,2,4,6,141,149,151],[1.0,38.0,1.0,71.2833,1.0,1.0,1.0,1.0])              |1       |\n",
      "|(159,[0,1,2,4,6,55,148,151],[1.0,35.0,1.0,53.1,1.0,1.0,1.0,1.0])                  |1       |\n",
      "|(159,[0,1,4,5,125,148,154],[1.0,54.0,51.8625,1.0,1.0,1.0,1.0])                    |0       |\n",
      "|(159,[0,1,2,3,4,6,102,148,157],[3.0,4.0,1.0,1.0,16.7,1.0,1.0,1.0,1.0])            |1       |\n",
      "|(159,[0,1,4,6,118,148,151],[1.0,58.0,26.55,1.0,1.0,1.0,1.0])                      |1       |\n",
      "|(159,[0,1,4,5,130,148,153],[2.0,34.0,13.0,1.0,1.0,1.0,1.0])                       |1       |\n",
      "|(159,[0,1,4,5,121,148,155],[1.0,28.0,35.5,1.0,1.0,1.0,1.0])                       |1       |\n",
      "|(159,[0,1,2,3,4,5,19,148,151],[1.0,19.0,3.0,2.0,263.0,1.0,1.0,1.0,1.0])           |0       |\n",
      "|(159,[0,1,2,4,6,96,149,152],[1.0,30.079501879699244,1.0,146.5208,1.0,1.0,1.0,1.0])|1       |\n",
      "|(159,[0,1,2,4,6,13,149,153],[1.0,49.0,1.0,76.7292,1.0,1.0,1.0,1.0])               |1       |\n",
      "|(159,[0,1,3,4,5,114,149,152],[1.0,65.0,1.0,61.9792,1.0,1.0,1.0,1.0])              |0       |\n",
      "|(159,[0,1,4,5,127,148,151],[1.0,30.079501879699244,35.5,1.0,1.0,1.0,1.0])         |1       |\n",
      "|(159,[0,1,2,4,5,83,148,151],[1.0,45.0,1.0,83.475,1.0,1.0,1.0,1.0])                |0       |\n",
      "|(159,[0,1,4,6,138,148,156],[2.0,29.0,10.5,1.0,1.0,1.0,1.0])                       |1       |\n",
      "|(159,[0,1,4,5,136,148,156],[3.0,25.0,7.65,1.0,1.0,1.0,1.0])                       |0       |\n",
      "|(159,[0,1,2,3,4,6,19,148,151],[1.0,23.0,3.0,2.0,263.0,1.0,1.0,1.0,1.0])           |1       |\n",
      "|(159,[0,1,2,4,5,147,148,154],[1.0,46.0,1.0,61.175,1.0,1.0,1.0,1.0])               |0       |\n",
      "|(159,[0,1,4,5,143,149,155],[1.0,71.0,34.6542,1.0,1.0,1.0,1.0])                    |0       |\n",
      "|(159,[0,1,3,4,5,144,149,153],[1.0,23.0,1.0,63.3583,1.0,1.0,1.0,1.0])              |1       |\n",
      "|(159,[0,1,3,4,5,80,148,153],[1.0,21.0,1.0,77.2875,1.0,1.0,1.0,1.0])               |0       |\n",
      "+----------------------------------------------------------------------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=['Pclass','Age','SibSp','Parch' ,'Fare','encoded_Sex','encoded_Ticket','encoded_Embarked','encoded_cabin_First'\n",
    "], outputCol=\"features\")\n",
    "\n",
    "\n",
    "final_df = assembler.transform(encoded_df)\n",
    "final_df.select(['features','Survived']).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dU8DeZfh7JIo"
   },
   "source": [
    "**Use randomSplit function and split data to x_train, and X_test with 80% and 20% Consecutive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "8C11xf1iAzKp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 256 rows in the training set, and 49 in the test set\n"
     ]
    }
   ],
   "source": [
    "trainDF, testDF = final_df.randomSplit([.8,.2],seed=42)\n",
    "print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0c_Hf_b0R12"
   },
   "source": [
    "**Pipeline: ML Pipelines provide a uniform set of high-level APIs built on top of DataFrames that help users create and tune practical machine learning pipelines.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IJQvmFai72O7"
   },
   "source": [
    "**Build RandomForestClassifier model and use pipeline to fit and transform then display \"prediction, Survived, features\" columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "YnpmZqlTLPGq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|            features|Survived|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|(159,[0,1,4,5,125...|       0|       1.0|\n",
      "|(159,[0,1,4,5,121...|       1|       1.0|\n",
      "|(159,[0,1,2,4,6,9...|       1|       1.0|\n",
      "|(159,[0,1,4,6,138...|       1|       1.0|\n",
      "|(159,[0,1,3,4,5,8...|       0|       1.0|\n",
      "|(159,[0,1,3,4,5,8...|       0|       1.0|\n",
      "|(159,[0,1,2,4,6,5...|       1|       1.0|\n",
      "|(159,[0,1,4,5,117...|       0|       1.0|\n",
      "|(159,[0,1,2,4,5,7...|       0|       1.0|\n",
      "|(159,[0,1,2,3,4,5...|       1|       1.0|\n",
      "|(159,[0,1,2,3,4,6...|       0|       1.0|\n",
      "|(159,[0,1,4,6,10,...|       1|       1.0|\n",
      "|(159,[0,1,5,108,1...|       0|       0.0|\n",
      "|(159,[0,1,2,4,6,1...|       1|       1.0|\n",
      "|(159,[0,1,4,6,129...|       1|       1.0|\n",
      "|(159,[0,1,2,3,4,6...|       1|       1.0|\n",
      "|(159,[0,1,4,5,119...|       0|       1.0|\n",
      "|(159,[0,1,4,5,64,...|       0|       1.0|\n",
      "|(159,[0,1,4,6,10,...|       1|       1.0|\n",
      "|(159,[0,1,4,6,87,...|       1|       1.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "Rand_classify = RandomForestClassifier(featuresCol='features',labelCol='Survived')\n",
    "\n",
    "#pipeline = Pipeline(stages=[indexer, encoder, assembler , Rand_classify])\n",
    "#model = pipeline.fit(train_df)\n",
    "#transformed = model.transform(test_df)\n",
    "#transformed.select(['Survived','prediction']).show(truncate=False)\n",
    "\n",
    "\n",
    "model = Rand_classify.fit(trainDF)\n",
    "# transform the data\n",
    "sample_data_test = model.transform(testDF)\n",
    "sample_data_test.select(['features','Survived' , 'prediction']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSXEI8-r8bKY"
   },
   "source": [
    "**Use MulticlassClassificationEvaluator and set the \"labelCol\" to \"Survived\",  \"predictionCol\" to \"prediction\", \"metricName\" to \"accuracy\"** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Rl0UAKCaBDO-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data = 0.693878\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "Accuracy = evaluator.evaluate(sample_data_test)\n",
    "print(\"Accuracy on test data = %g\" % Accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO6_R1zJ9R1R"
   },
   "source": [
    "**When you are finished send the project via Google classroom**\n",
    "**Please let me know if you have any questions.**\n",
    "* nabieh.mostafa@yahoo.com\n",
    "* +201015197566 (Whatsapp)\n",
    "\n",
    "**Don't Hate me, I push you to learn**\n",
    "\n",
    "**I will help you to become an awesome data engineer.**\n",
    "\n",
    "**Why did I say that \"Data Engineer\"?**\n",
    "\n",
    "**Tricky question, but an optional question, if you would like to know the answer, ask me.**\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Practical_work.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
