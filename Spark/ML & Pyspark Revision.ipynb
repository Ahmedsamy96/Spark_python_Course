{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "from pyspark.sql.types import StringType , IntegerType , StructField , StructType \n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+---+----+-------+----+--------+---+-----+\n",
      "|pregnant|glucose| bp|skin|insulin| bmi|pedigree|age|label|\n",
      "+--------+-------+---+----+-------+----+--------+---+-----+\n",
      "|       6|    148| 72|  35|      0|33.6|   0.627| 50|    1|\n",
      "|       1|     85| 66|  29|      0|26.6|   0.351| 31|    0|\n",
      "|       8|    183| 64|   0|      0|23.3|   0.672| 32|    1|\n",
      "|       1|     89| 66|  23|     94|28.1|   0.167| 21|    0|\n",
      "|       0|    137| 40|  35|    168|43.1|   2.288| 33|    1|\n",
      "|       5|    116| 74|   0|      0|25.6|   0.201| 30|    0|\n",
      "|       3|     78| 50|  32|     88|31.0|   0.248| 26|    1|\n",
      "|      10|    115|  0|   0|      0|35.3|   0.134| 29|    0|\n",
      "|       2|    197| 70|  45|    543|30.5|   0.158| 53|    1|\n",
      "|       8|    125| 96|   0|      0| 0.0|   0.232| 54|    1|\n",
      "|       4|    110| 92|   0|      0|37.6|   0.191| 30|    0|\n",
      "|      10|    168| 74|   0|      0|38.0|   0.537| 34|    1|\n",
      "|      10|    139| 80|   0|      0|27.1|   1.441| 57|    0|\n",
      "|       1|    189| 60|  23|    846|30.1|   0.398| 59|    1|\n",
      "|       5|    166| 72|  19|    175|25.8|   0.587| 51|    1|\n",
      "|       7|    100|  0|   0|      0|30.0|   0.484| 32|    1|\n",
      "|       0|    118| 84|  47|    230|45.8|   0.551| 31|    1|\n",
      "|       7|    107| 74|   0|      0|29.6|   0.254| 31|    1|\n",
      "|       1|    103| 30|  38|     83|43.3|   0.183| 33|    0|\n",
      "|       1|    115| 70|  30|     96|34.6|   0.529| 32|    1|\n",
      "+--------+-------+---+----+-------+----+--------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('diabetes.csv' ,header = False , inferSchema =True)\n",
    "df = df.toDF('pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pregnant: integer (nullable = true)\n",
      " |-- glucose: integer (nullable = true)\n",
      " |-- bp: integer (nullable = true)\n",
      " |-- skin: integer (nullable = true)\n",
      " |-- insulin: integer (nullable = true)\n",
      " |-- bmi: double (nullable = true)\n",
      " |-- pedigree: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 645 rows in the training set, and 123 in the test set\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test sets (20% held out for testing)\n",
    "trainDF, testDF = df.randomSplit([.8,.2],seed=42)\n",
    "print(f\"There are {trainDF.count()} rows in the training set, and {testDF.count()} in the test set\")\n",
    "\n",
    "trainX = trainDF.drop(\"label\")\n",
    "testX = testDF.drop(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-----+\n",
      "|features                                   |label|\n",
      "+-------------------------------------------+-----+\n",
      "|[6.0,148.0,72.0,35.0,0.0,33.6,0.627,50.0]  |1    |\n",
      "|[1.0,85.0,66.0,29.0,0.0,26.6,0.351,31.0]   |0    |\n",
      "|[8.0,183.0,64.0,0.0,0.0,23.3,0.672,32.0]   |1    |\n",
      "|[1.0,89.0,66.0,23.0,94.0,28.1,0.167,21.0]  |0    |\n",
      "|[0.0,137.0,40.0,35.0,168.0,43.1,2.288,33.0]|1    |\n",
      "|[5.0,116.0,74.0,0.0,0.0,25.6,0.201,30.0]   |0    |\n",
      "|[3.0,78.0,50.0,32.0,88.0,31.0,0.248,26.0]  |1    |\n",
      "|[10.0,115.0,0.0,0.0,0.0,35.3,0.134,29.0]   |0    |\n",
      "|[2.0,197.0,70.0,45.0,543.0,30.5,0.158,53.0]|1    |\n",
      "|[8.0,125.0,96.0,0.0,0.0,0.0,0.232,54.0]    |1    |\n",
      "|[4.0,110.0,92.0,0.0,0.0,37.6,0.191,30.0]   |0    |\n",
      "|[10.0,168.0,74.0,0.0,0.0,38.0,0.537,34.0]  |1    |\n",
      "|[10.0,139.0,80.0,0.0,0.0,27.1,1.441,57.0]  |0    |\n",
      "|[1.0,189.0,60.0,23.0,846.0,30.1,0.398,59.0]|1    |\n",
      "|[5.0,166.0,72.0,19.0,175.0,25.8,0.587,51.0]|1    |\n",
      "|[7.0,100.0,0.0,0.0,0.0,30.0,0.484,32.0]    |1    |\n",
      "|[0.0,118.0,84.0,47.0,230.0,45.8,0.551,31.0]|1    |\n",
      "|[7.0,107.0,74.0,0.0,0.0,29.6,0.254,31.0]   |1    |\n",
      "|[1.0,103.0,30.0,38.0,83.0,43.3,0.183,33.0] |0    |\n",
      "|[1.0,115.0,70.0,30.0,96.0,34.6,0.529,32.0] |1    |\n",
      "+-------------------------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age'], outputCol=\"features\")\n",
    "final_df = assembler.transform(df)\n",
    "\n",
    "vectTrainDF = assembler.transform(trainDF)\n",
    "\n",
    "final_df.select(['features','label']).show(truncate=False)\n",
    "final_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a GBT model.\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "features does not exist. Available: pregnant, glucose, bp, skin, insulin, bmi, pedigree, age, label",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e29c71ec235a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Make predictions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# Select example rows to display.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Params must be a param map but got %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_transform\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: features does not exist. Available: pregnant, glucose, bp, skin, insulin, bmi, pedigree, age, label"
     ]
    }
   ],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = gbt.fit(vectTrainDF)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testDF)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[assembler,gbt])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+----------+\n",
      "|            features|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|(8,[1,5,6,7],[73....|    0|       0.0|\n",
      "|[0.0,84.0,82.0,31...|    0|       0.0|\n",
      "|[0.0,91.0,68.0,32...|    0|       0.0|\n",
      "|(8,[1,6,7],[94.0,...|    0|       0.0|\n",
      "|[0.0,98.0,82.0,15...|    0|       0.0|\n",
      "|[0.0,101.0,62.0,0...|    0|       0.0|\n",
      "|[0.0,102.0,75.0,2...|    0|       0.0|\n",
      "|[0.0,105.0,64.0,4...|    0|       0.0|\n",
      "|[0.0,111.0,65.0,0...|    0|       0.0|\n",
      "|[0.0,113.0,76.0,0...|    1|       0.0|\n",
      "|[0.0,113.0,80.0,1...|    0|       0.0|\n",
      "|(8,[1,5,6,7],[117...|    0|       1.0|\n",
      "|[0.0,117.0,80.0,3...|    0|       0.0|\n",
      "|[0.0,119.0,64.0,1...|    0|       0.0|\n",
      "|[0.0,124.0,70.0,2...|    1|       1.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select('features','label','prediction').show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is :  0.7479674796747967\n",
      "Test Error = 0.252033\n",
      "GBTClassificationModel: uid = GBTClassifier_f0c3319a10e9, numTrees=10, numClasses=2, numFeatures=8\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predDF)\n",
    "print(\"accuracy is : \",accuracy)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "\n",
    "gbtModel = pipelineModel.stages[1]\n",
    "print(gbtModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
