{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "spark = (SparkSession.builder.getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType , IntegerType , StructField , StructType "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField('Author' , StringType() , False),\n",
    "                    StructField('Title' , StringType() , False),\n",
    "                    StructField('PagesNum' , IntegerType() , False)\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another method for creating a database schema : DDL method\n",
    "# schema = 'Author STRING NULLABLE=false , Title STRING , PagesNum INT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- URL: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- hits: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myschema = \" id INT ,First STRING  ,Last STRING , URL STRING , Published STRING ,hits INT\"\n",
    "# data = list of lists\n",
    "data = [[1, 'ahmed'  , 'samy' , 'ahmed.samy@x.com'  , 'my name is ahmed',254],\n",
    "        [2, 'hossam' , 'samy' , 'hossam.samy@y.com' , 'my name is hossam',879],\n",
    "        [3, 'Ali'    , 'samy' , 'ali.samy@z.com'    , 'my name is ali',452]]\n",
    "df = spark.createDataFrame(data , myschema)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----+-----------------+-----------------+----+\n",
      "| id| First|Last|              URL|        Published|hits|\n",
      "+---+------+----+-----------------+-----------------+----+\n",
      "|  1| ahmed|samy| ahmed.samy@x.com| my name is ahmed| 254|\n",
      "|  2|hossam|samy|hossam.samy@y.com|my name is hossam| 879|\n",
      "|  3|   Ali|samy|   ali.samy@z.com|   my name is ali| 452|\n",
      "+---+------+----+-----------------+-----------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|hits Squared|\n",
      "+------------+\n",
      "|         508|\n",
      "|        1758|\n",
      "|         904|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df.select(expr('hits * 2').alias('hits Squared')).show()\n",
    "# (truncate =False): show all content when show record in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.csv('NullData.csv' ,header = True , inferSchema =True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| null|\n",
      "|emp2| null| null|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(thresh=1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset='Sales').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(subset=['Name','Sales']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John|400.5|\n",
      "|emp2| null|400.5|\n",
      "|emp3| null|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fill_with_mean(df, exclude=set()): \n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c not in exclude\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "fill_with_mean(df, [\"Id\", \"Name\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|CallNumber|UnitID|IncidentNumber|        CallType|  CallDate| WatchDate|CallFinalDisposition|       AvailableDtTm|             Address|City|Zipcode|Battalion|StationArea| Box|OriginalPriority|Priority|FinalPriority|ALSUnit|CallTypeGroup|NumAlarms|UnitType|UnitSequenceInCallDispatch|FirePreventionDistrict|SupervisorDistrict|        Neighborhood|            Location|        RowID|    Delay|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "|  20110016|   T13|       2003235|  Structure Fire|01/11/2002|01/10/2002|               Other|01/11/2002 01:51:...|2000 Block of CAL...|  SF|  94109|      B04|         38|3362|               3|       3|            3|  false|         null|        1|   TRUCK|                         2|                     4|                 5|     Pacific Heights|(37.7895840679362...|020110016-T13|     2.95|\n",
      "|  20110022|   M17|       2003241|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 03:01:...|0 Block of SILVER...|  SF|  94124|      B10|         42|6495|               3|       3|            3|   true|         null|        1|   MEDIC|                         1|                    10|                10|Bayview Hunters P...|(37.7337623673897...|020110022-M17|      4.7|\n",
      "|  20110023|   M41|       2003242|Medical Incident|01/11/2002|01/10/2002|               Other|01/11/2002 02:39:...|MARKET ST/MCALLIS...|  SF|  94102|      B03|         01|1455|               3|       3|            3|   true|         null|        1|   MEDIC|                         2|                     3|                 6|          Tenderloin|(37.7811772186856...|020110023-M41|2.4333334|\n",
      "+----------+------+--------------+----------------+----------+----------+--------------------+--------------------+--------------------+----+-------+---------+-----------+----+----------------+--------+-------------+-------+-------------+---------+--------+--------------------------+----------------------+------------------+--------------------+--------------------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datapath= 'sf-fire-calls.csv'\n",
    "df_fire=spark.read.csv(datapath ,header = True , inferSchema =True)\n",
    "df_fire.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CallNumber: integer (nullable = true)\n",
      " |-- UnitID: string (nullable = true)\n",
      " |-- IncidentNumber: integer (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- CallDate: string (nullable = true)\n",
      " |-- WatchDate: string (nullable = true)\n",
      " |-- CallFinalDisposition: string (nullable = true)\n",
      " |-- AvailableDtTm: string (nullable = true)\n",
      " |-- Address: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Zipcode: integer (nullable = true)\n",
      " |-- Battalion: string (nullable = true)\n",
      " |-- StationArea: string (nullable = true)\n",
      " |-- Box: string (nullable = true)\n",
      " |-- OriginalPriority: string (nullable = true)\n",
      " |-- Priority: string (nullable = true)\n",
      " |-- FinalPriority: integer (nullable = true)\n",
      " |-- ALSUnit: boolean (nullable = true)\n",
      " |-- CallTypeGroup: string (nullable = true)\n",
      " |-- NumAlarms: integer (nullable = true)\n",
      " |-- UnitType: string (nullable = true)\n",
      " |-- UnitSequenceInCallDispatch: integer (nullable = true)\n",
      " |-- FirePreventionDistrict: string (nullable = true)\n",
      " |-- SupervisorDistrict: string (nullable = true)\n",
      " |-- Neighborhood: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- RowID: string (nullable = true)\n",
      " |-- Delay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-------------------------------------+\n",
      "|Neighborhood                  |Location                             |\n",
      "+------------------------------+-------------------------------------+\n",
      "|Pacific Heights               |(37.7895840679362, -122.428071912459)|\n",
      "|Bayview Hunters Point         |(37.7337623673897, -122.396113802632)|\n",
      "|Tenderloin                    |(37.7811772186856, -122.411699931232)|\n",
      "|Bernal Heights                |(37.7388432849018, -122.423948785199)|\n",
      "|Western Addition              |(37.7872890372638, -122.424236212664)|\n",
      "|Financial District/South Beach|(37.7886866619654, -122.392722833778)|\n",
      "|Oceanview/Merced/Ingleside    |(37.7140353531157, -122.454117149916)|\n",
      "|Tenderloin                    |(37.7826266328595, -122.41915582123) |\n",
      "|Japantown                     |(37.784958590666, -122.431435274503) |\n",
      "|Castro/Upper Market           |(37.7618954753708, -122.437298717721)|\n",
      "|Excelsior                     |(37.7105545807996, -122.443335369545)|\n",
      "|Nob Hill                      |(37.7881263034393, -122.417657214041)|\n",
      "|Outer Richmond                |(37.7850084431077, -122.480723607753)|\n",
      "|West of Twin Peaks            |(37.7501117393668, -122.460819155469)|\n",
      "|Inner Richmond                |(37.7768682293368, -122.472039541478)|\n",
      "|Inner Richmond                |(37.7768682293368, -122.472039541478)|\n",
      "|Castro/Upper Market           |(37.7635007029742, -122.434209629009)|\n",
      "|North Beach                   |(37.7980228452184, -122.405863212632)|\n",
      "|Inner Sunset                  |(37.7585821585787, -122.453613744703)|\n",
      "|Tenderloin                    |(37.7854670505017, -122.415977627827)|\n",
      "+------------------------------+-------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fire.select(['Neighborhood','Location']).where(col('Neighborhood') != 'Mission' ).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_data= 'departuredelays.csv'\n",
    "df_air=spark.read.csv(airline_data ,header = True , inferSchema =True)\n",
    "df_air.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1311525|    6|     137|   ABI|        DFW|\n",
      "|1090700|    6|     137|   ABI|        DFW|\n",
      "|1181400|    6|     137|   ABI|        DFW|\n",
      "|1241520|    6|     494|   ABQ|        DFW|\n",
      "|1021326|    6|    1103|   ABQ|        ATL|\n",
      "|1031326|    6|    1103|   ABQ|        ATL|\n",
      "|1031440|    6|     852|   ABQ|        MSP|\n",
      "|1310545|    6|     589|   ABQ|        LAX|\n",
      "|1231635|    6|     589|   ABQ|        LAX|\n",
      "|1060955|    6|     504|   ABQ|        DAL|\n",
      "|1070830|    6|     660|   ABQ|        HOU|\n",
      "|1081305|    6|     423|   ABQ|        LAS|\n",
      "|1081915|    6|     504|   ABQ|        DAL|\n",
      "|1091620|    6|     285|   ABQ|        PHX|\n",
      "|1091155|    6|     589|   ABQ|        LAX|\n",
      "|1101415|    6|     660|   ABQ|        HOU|\n",
      "|1101315|    6|     285|   ABQ|        PHX|\n",
      "|1110935|    6|     660|   ABQ|        HOU|\n",
      "|1130920|    6|     285|   ABQ|        PHX|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# it's benefit that you can use a sql code like normal way. \n",
    "df_air.createOrReplaceTempView('us_flight_delay')\n",
    "spark.sql(\"\"\"select * from us_flight_delay \n",
    "              where delay=6 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard way for reading df file(csv , json , xpath , parquet)\n",
    "# .schema(schema)\n",
    "#new_path = 'C:\\Users\\SAMY\\Desktop\\Spark\\Day2'\n",
    "#df = spark.read.format('csv').option('header','True').option('inferSchema' ,'True').load(airline_data)\n",
    "#df.write.format(parquet).mode('overwrite').save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
